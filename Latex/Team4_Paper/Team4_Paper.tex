\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{multirow}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{TIME SERIES ANALYSIS FOR REAL ESTATE STOCK PRICE PREDICTION USING ML/DL ALGORITHMS METHODS
}
\newcommand{\linebreakand}{%
  \end{@IEEEauthorhalign}
  \hfill\mbox{}\par
  \mbox{}\hfill\begin{@IEEEauthorhalign}
}

\author{\IEEEauthorblockN{1\textsuperscript{st} Ngo Thuy Yen Nhi}
\IEEEauthorblockA{\textit{Team 4 - IS403.O21} \\
Ho Chi Minh City, Viet Nam \\
e-mail: 21521230@gm.uit.edu.vn}
\and
\IEEEauthorblockN{2\textsuperscript{nd} Nguyen Duong}
\IEEEauthorblockA{\textit{Team 4 - IS403.O21} \\
Ho Chi Minh City, Viet Nam \\
e-mail: 21521990@gm.uit.edu.vn}
\and
\IEEEauthorblockN{3\textsuperscript{rd} Pham Duy Khanh}
\IEEEauthorblockA{\textit{Team 4 - IS403.O21} \\
Ho Chi Minh City, Viet Nam \\
e-mail: 21522211@gm.uit.edu.vn}
\and
\linebreakand
    \IEEEauthorblockN{4\textsuperscript{th} Nguyen Ngoc Ha My}
    \IEEEauthorblockA{\textit{Team 4 - IS403.O21} \\
    Ho Chi Minh City, Viet Nam \\
    e-mail: 21522351@gm.uit.edu.vn}
\and
    \IEEEauthorblockN{5\textsuperscript{th} Le Thuan Hieu}
    \IEEEauthorblockA{\textit{Team 4 - IS403.O21} \\
    Ho Chi Minh City, Viet Nam \\
    e-mail: 21522072@gm.uit.edu.vn}
}

%======== Hieu & Nhi



%My-KalmanFilter
\subsection{Kalman Filter}

The Kalman Filter is a linear-gaussian state space model used for time series prediction. It was developed by Rudolf E. Kálmán in 1960, especially to handle noisy and fluctuating models. It improves the accuracy of predictions in real-time applications. The main idea of this algorithm is to combine uncertain information from the current time with noisy environmental data in order to create more confident predictions of future events.

This is how Kalman filter works:

Initial estimation: Initialize the initial state $\widehat{x}_{0,0}$ and the initial state covariance matrix $P_{0,0}$.

Iterative process of prediction and measurement update:

\begin{enumerate}
    \item Prediction
    \begin{itemize}
        \item Prediction of the next state
        \[ \widehat{x}_{n + 1, n} = F\widehat{x}_{n, n} + Gu_{n} \]
        \item Prediction of the next state uncertainty (error)
        \[ P_{n + 1, n} = FP_{n, n}F^{T} + Q \]
    \end{itemize}
    \item Measurement update
    \begin{itemize}
        \item Calculation of the Kalman Gain prediction weight
        \[ K_{n} = P_{n, n - 1}H^{T}(HP_{n, n - 1}H^{T} + R_{n})^{-1} \]
        \item Update of the state estimate
        \[ \widehat{x}_{n, n} = \widehat{x}_{n, n - 1} + K_{n}(Z_{n} - H\widehat{x}_{n, n - 1}) \]
        \item Update of the estimate uncertainty (error)
        \[ P_{n, n} = (1 - K_{n}H)P_{n, n - 1}(1 - K_{n}H)^{T} + K_{n}R_{n}K_{n}^{T} \]
    \end{itemize}

Where:
    \begin{itemize}
        \item $x$: The state actor
        \item $F$: The state transition matrix
        \item $G$: The control matrix
        \item $u$: The input variable
        \item $P$: The covariance matrix
        \item $H$: The observation matrix
        \item ${H}^{T}$: The transpose of the observation matrix
        \item $K$: The Kalman Gain
        \item $R$: The measurement covariance matrix
        \item $z$: The vector measurement
    \end{itemize}
\end{enumerate}
%======== Khanh ==== TBATS
\subsection{TBATS}  
TBATS models are a sophisticated class of time series models that integrate several techniques to address complex data patterns. 

TBATS employs trigonometric functions to model multiple seasonalities simultaneously, such as daily, weekly, and annual cycles. Moreover, a Box-Cox transformation is applied to stabilize the variance, making the data more suitable for modeling.

TBATS models also incorporate ARMA components to manage short-term dynamics and autocorrelations in the residuals, thereby improving forecast accuracy. Additionally, they allow for damping trends, accommodating trends that decrease over time. These features enable TBATS models to effectively handle complex seasonal patterns, nonlinearities, and residual autocorrelations, making them particularly useful for forecasting data with intricate seasonal structures, such as in the case study of forecasting the second wave of the COVID-19 epidemic case study.\cite{b5}

\begin{enumerate}
    \item \textbf{Box-Cox Transformation (BATS)}:
    \[
    y_{t}^{(\omega)} = \begin{cases} 
    \frac{y_t^\omega - 1}{\omega} & \text{if } \omega \neq 0 \\
    \ln(y_t) & \text{if } \omega = 0 
    \end{cases}
    \]
    \item \textbf{Trend Component}:
    \[
    l_{t} = \ell_{t-1} + \phi \, b_{t-1} + \alpha \, d_{t}  
    \]
    \[
    b_{t} = \phi \, b_{t-1} + \beta \, d_{t}
    \]
    \item \textbf{Seasonal Component}:
    \[
    s_{t}^{(i)} = \sum_{j=1}^{(k_i)} s_{j,t}^{(i)}
    \]
    \[
    s_{j,t}^{(i)} = s_{j,t-1}^{(i)} \cos \left( \omega_i \right) + s_{j,t-1}^{(i)} \sin \left( \omega_i \right) + \gamma_{1}^{(i)} \, d_{t}
    \]
    \[
    s_{j,t}^{(i)} = - s_{j,t-1}^{(i)} \sin \left( \omega_i \right) + s_{j,t-1}^{(i)} \cos \left( \omega_i \right) + \gamma_{2}^{(i)} \, d_{t}
    \]
    \[
    \omega_i = \frac{2 \pi j}{m_i}
    \]
    \item \textbf{Error Component (ARMA)}:
    \[
    d_{t} = \sum_{i=1}^{p} \phi_{i} \, d_{t-1} + \sum_{i=1}^{q} \theta_{i} \, e_{t-1} + e_{t}
    \]
    \item \textbf{Overall TBATS Model}:
    \[
    y_{t}^{(\omega)} = \ell_{t-1} + \phi \, b_{t-1} + \sum_{i=1}^{T} s_{t - m_i}^{(i)} + d_t
    \]

\textbf{Where:}
    \begin{itemize}
        \item  \({\omega}\) is the box-cox transformation.
        \item \( y_{t} \) is the observation at time \( t \).
        \item \( \ell_{t} \) is the local level in period \( t \).
        \item \( b_{t} \) is the short-run trend in period \( t \).
        \item \( s_{t}^{(i)} \) is the seasonal component at time \( t \).
        \item \( d_{t} \) is an ARMA(p,q) process.
        \item \( m_i \) is the length of the \( i \)-th seasonal period.
        \item \( T \) is the total number of seasonal patterns.
        \item \( \phi \) is the damping parameter.
        \item \( \omega_i \) is the frequency parameter for the \( i \)-th seasonal period, calculated as \( \omega_i = \frac{2 \pi j}{m_i} \), where \( j \) is an integer representing the harmonic and \( m_i \) is the length of the seasonal period.
        \item \( \alpha, \beta \) are smoothing parameters.
        \item \( \phi_i, \theta_i \) are ARMA(p,q) coefficients.
        \item \( e_{t} \) is Gaussian white noise.
        \item \( \gamma_1, \gamma_2 \) are seasonal smoothing parameters (two for each period).
    \end{itemize}
\end{enumerate}





%======= Duong
% Dương
\subsection{ResNet}  
ResNet (Residual Network) is a type of deep neural network that utilizes residual blocks to enhance performance and mitigate the vanishing gradient problem as the network becomes very deep. Each residual block comprises three convolutional blocks with different kernel sizes, combined with a shortcut connection that adds the initial input to the final output of the block. This ResNet model includes three residual blocks, followed by a global average pooling layer and a fully connected (fc) layer to produce the final output. \cite{b6}

\begin{enumerate}
  \item Equations
  
  Assume $x$ is the initial input of the residual block. Each convolutional block performs the following transformations:
  \begin{align*}
    h_1 &= \text{ReLU}(\text{BN}(\text{Conv1d}(x))) \\
    h_2 &= \text{ReLU}(\text{BN}(\text{Conv1d}(h_1))) \\
    h_3 &= \text{BN}(\text{Conv1d}(h_2))
  \end{align*}

  The output of the residual block is calculated by adding the initial input $x$ to $h_3$ and then applying the ReLU function:
  \begin{align*}
    y &= h_3 + x \\
    \hat{h} &= \text{ReLU}(y)
  \end{align*}

  The number of filters for the residual blocks is $k_i = \{64, 128, 128\}$.

  \vspace{0.5cm}  % Optional vertical spacing

  ResNet Structure:

  The ResNet structure is as follows: \\
  \begin{align*}
    \text{ResNet} &= [\text{ResBlock1}, \text{ResBlock2}, \\
                    &\quad \text{ResBlock3}, \text{GlobalAvgPooling}, \text{FC}] \cite{b6}
  \end{align*}
    
  \item ResNetLSTM
  
    ResNetLSTM combines the ResNet architecture with LSTM (Long Short-Term Memory) to handle time series data. After the inputs are processed through the three residual blocks of ResNet, the output is transformed to be suitable for the LSTM input. LSTM has the ability to remember information across time steps, making the model better suited for handling time series data. Finally, a fully connected layer is used to produce the final output. \cite{b7} \\ 
    The output of ResNet is calculated as described above. The output from the last residual block is transformed to fit the LSTM input:

    \begin{align*}
        x_{\text{lstm}} = x.\text{transpose}(1, 2)
    \end{align*}
    
    The LSTM processes the input and returns the output at the last time step:
    \begin{align*}
      x_{\text{lstm}}, (h_n, c_n) = \text{LSTM}(x_{\text{lstm}})
    \end{align*}
    \begin{align*}
      x_{\text{final}} = x_{\text{lstm}}[:, -1, :]
    \end{align*}
    The final output is computed by the fully connected layer:    \begin{align*}
      y = \text{FC}(x_{\text{final}})
    \end{align*}

     
     The ResNetLSTM structure is as follows: 
     \begin{align*}
        \text{ResNetLSTM} = [\text{ResBlock1}, \text{ResBlock2},\\ \text{ResBlock3}, \text{LSTM}, \text{FC}]
    \end{align*}
  \item Summary \\
    ResNet focuses on using residual blocks to increase the depth of the neural network and mitigate the vanishing gradient problem. ResNetLSTM combines the power of ResNet in handling spatial features with the capability of LSTM in remembering and processing temporal features.\\
    The main difference between the two models lies in the final part of each model:
    \subsubsection{ResNet}
    \begin{itemize}
      \item Uses a global average pooling layer: \\
        \text{self.global\_avg\_pool} = \text{nn.AdaptiveAvgPool1d}(1)
      \item The output from the global average pooling layer is fed into the fully connected layer: \\
        $x = \text{self.fc}(x)$
    \end{itemize}
    
    \subsubsection{ResNetLSTM}
    \begin{itemize}
      \item Uses an LSTM layer: 
        \begin{align*}
          \text{self.lstm} &= \text{nn.LSTM}(nf \times 2, \text{lstm\_hidden\_units}, \\
                                          &\quad \text{num\_lstm\_layers}, \text{batch\_first=True})
        \end{align*}
      \item The output from the LSTM is taken at the last time step and fed into the fully connected layer: 
        \begin{align*}
          x &= x[:, -1, :] \\
          x &= \text{self.fc}(x)
        \end{align*}
    \end{itemize}

\end{enumerate}















