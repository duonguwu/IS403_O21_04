\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{multirow}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{TIME SERIES ANALYSIS FOR REAL ESTATE STOCK PRICE PREDICTION USING ML/DL ALGORITHMS METHODS
}
\newcommand{\linebreakand}{%
  \end{@IEEEauthorhalign}
  \hfill\mbox{}\par
  \mbox{}\hfill\begin{@IEEEauthorhalign}
}

\author{\IEEEauthorblockN{1\textsuperscript{st} Ngo Thuy Yen Nhi}
\IEEEauthorblockA{\textit{Team 4 - IS403.O21} \\
Ho Chi Minh City, Viet Nam \\
e-mail: 21521230@gm.uit.edu.vn}
\and
\IEEEauthorblockN{2\textsuperscript{nd} Nguyen Duong}
\IEEEauthorblockA{\textit{Team 4 - IS403.O21} \\
Ho Chi Minh City, Viet Nam \\
e-mail: 21521990@gm.uit.edu.vn}
\and
\IEEEauthorblockN{3\textsuperscript{rd} Pham Duy Khanh}
\IEEEauthorblockA{\textit{Team 4 - IS403.O21} \\
Ho Chi Minh City, Viet Nam \\
e-mail: 21522211@gm.uit.edu.vn}
\and
\linebreakand
    \IEEEauthorblockN{4\textsuperscript{th} Nguyen Ngoc Ha My}
    \IEEEauthorblockA{\textit{Team 4 - IS403.O21} \\
    Ho Chi Minh City, Viet Nam \\
    e-mail: 21522351@gm.uit.edu.vn}
\and
    \IEEEauthorblockN{5\textsuperscript{th} Le Thuan Hieu}
    \IEEEauthorblockA{\textit{Team 4 - IS403.O21} \\
    Ho Chi Minh City, Viet Nam \\
    e-mail: 21522072@gm.uit.edu.vn}
}

%======== Hieu & Nhi




%======== Khanh ==== TBATS






%======= Duong
% Dương
\subsection{ResNet}  
ResNet (Residual Network) is a type of deep neural network that utilizes residual blocks to enhance performance and mitigate the vanishing gradient problem as the network becomes very deep. Each residual block comprises three convolutional blocks with different kernel sizes, combined with a shortcut connection that adds the initial input to the final output of the block. This ResNet model includes three residual blocks, followed by a global average pooling layer and a fully connected (fc) layer to produce the final output. \cite{b6}

\begin{enumerate}
  \item Equations
  
  Assume $x$ is the initial input of the residual block. Each convolutional block performs the following transformations:
  \begin{align*}
    h_1 &= \text{ReLU}(\text{BN}(\text{Conv1d}(x))) \\
    h_2 &= \text{ReLU}(\text{BN}(\text{Conv1d}(h_1))) \\
    h_3 &= \text{BN}(\text{Conv1d}(h_2))
  \end{align*}

  The output of the residual block is calculated by adding the initial input $x$ to $h_3$ and then applying the ReLU function:
  \begin{align*}
    y &= h_3 + x \\
    \hat{h} &= \text{ReLU}(y)
  \end{align*}

  The number of filters for the residual blocks is $k_i = \{64, 128, 128\}$.

  \vspace{0.5cm}  % Optional vertical spacing

  ResNet Structure:

  The ResNet structure is as follows: \\
  \begin{align*}
    \text{ResNet} &= [\text{ResBlock1}, \text{ResBlock2}, \\
                    &\quad \text{ResBlock3}, \text{GlobalAvgPooling}, \text{FC}] \cite{b6}
  \end{align*}
    
  \item ResNetLSTM
  
    ResNetLSTM combines the ResNet architecture with LSTM (Long Short-Term Memory) to handle time series data. After the inputs are processed through the three residual blocks of ResNet, the output is transformed to be suitable for the LSTM input. LSTM has the ability to remember information across time steps, making the model better suited for handling time series data. Finally, a fully connected layer is used to produce the final output. \cite{b7} \\ 
    The output of ResNet is calculated as described above. The output from the last residual block is transformed to fit the LSTM input:

    \begin{align*}
        x_{\text{lstm}} = x.\text{transpose}(1, 2)
    \end{align*}
    
    The LSTM processes the input and returns the output at the last time step:
    \begin{align*}
      x_{\text{lstm}}, (h_n, c_n) = \text{LSTM}(x_{\text{lstm}})
    \end{align*}
    \begin{align*}
      x_{\text{final}} = x_{\text{lstm}}[:, -1, :]
    \end{align*}
    The final output is computed by the fully connected layer:    \begin{align*}
      y = \text{FC}(x_{\text{final}})
    \end{align*}

     
     The ResNetLSTM structure is as follows: 
     \begin{align*}
        \text{ResNetLSTM} = [\text{ResBlock1}, \text{ResBlock2},\\ \text{ResBlock3}, \text{LSTM}, \text{FC}]
    \end{align*}
  \item Summary \\
    ResNet focuses on using residual blocks to increase the depth of the neural network and mitigate the vanishing gradient problem. ResNetLSTM combines the power of ResNet in handling spatial features with the capability of LSTM in remembering and processing temporal features.\\
    The main difference between the two models lies in the final part of each model:
    \subsubsection{ResNet}
    \begin{itemize}
      \item Uses a global average pooling layer: \\
        \text{self.global\_avg\_pool} = \text{nn.AdaptiveAvgPool1d}(1)
      \item The output from the global average pooling layer is fed into the fully connected layer: \\
        $x = \text{self.fc}(x)$
    \end{itemize}
    
    \subsubsection{ResNetLSTM}
    \begin{itemize}
      \item Uses an LSTM layer: 
        \begin{align*}
          \text{self.lstm} &= \text{nn.LSTM}(nf \times 2, \text{lstm\_hidden\_units}, \\
                                          &\quad \text{num\_lstm\_layers}, \text{batch\_first=True})
        \end{align*}
      \item The output from the LSTM is taken at the last time step and fed into the fully connected layer: 
        \begin{align*}
          x &= x[:, -1, :] \\
          x &= \text{self.fc}(x)
        \end{align*}
    \end{itemize}

\end{enumerate}















